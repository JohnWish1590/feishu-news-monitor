import feedparser
import requests
import json
import time
import os
from datetime import datetime, timedelta, timezone
from deep_translator import GoogleTranslator

# ================= é…ç½®åŒº =================
FEISHU_WEBHOOK = os.environ.get("FEISHU_WEBHOOK")
KEYWORD = "ç›‘æ§"

# âš ï¸ æµ‹è¯•å®Œè®°å¾—æŠŠè¿™ä¸ªæ”¹å› 16
TIME_WINDOW_MINUTES = 1440 

def load_rss_list():
    rss_list = []
    if os.path.exists("rss.txt"):
        with open("rss.txt", "r", encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    rss_list.append(line)
    return rss_list

RSS_LIST = load_rss_list()
# =========================================

def is_work_time():
    utc_now = datetime.now(timezone.utc)
    beijing_time = utc_now + timedelta(hours=8)
    if 8 <= beijing_time.hour < 22:
        return True
    return False

def translate_text(text):
    try:
        for char in text:
            if '\u4e00' <= char <= '\u9fff': return text
        translator = GoogleTranslator(source='auto', target='zh-CN')
        return translator.translate(text)
    except: return text

# --- æ–°å¢åŠŸèƒ½ï¼šæŠŠæ–°é—»å†™å…¥ index.html (æ”¯æŒå€’åº) ---
def update_html_archive(news_list):
    """è¯»å– index.htmlï¼ŒæŠŠæ–°æ–°é—»æ’å…¥åˆ°æ ‡è®°ä½"""
    if not os.path.exists("index.html"): return
    
    # 1. ç”Ÿæˆæ–°å†…å®¹çš„ HTML ç‰‡æ®µ
    new_html = ""
    for news in news_list:
        # HTML å¡ç‰‡æ ·å¼
        card = f"""
        <a href="{news['link']}" target="_blank">
            <div class="news-card">
                <div class="news-header">
                    <span class="source-tag">{news['source']}</span>
                    <span class="time-tag">{news['display_time']}</span>
                </div>
                <div class="news-title">{news['title_cn']}</div>
                <div class="news-meta">åŸæ–‡ï¼š{news['title']}</div>
            </div>
        </a>
        """
        new_html += card

    # 2. è¯»å–åŸæ–‡ä»¶å¹¶æ’å…¥
    with open("index.html", "r", encoding="utf-8") as f:
        content = f.read()
    
    # å…³é”®ç‚¹ï¼šæ‰¾åˆ°æ ‡è®°ä½ï¼ŒæŠŠæ–°å†…å®¹æ’åœ¨æ ‡è®°åé¢
    # å› ä¸ºæˆ‘ä»¬ä¼ å…¥çš„ list å·²ç»æ˜¯ã€æ–°->æ—§ã€‘æ’åºçš„ï¼Œæ‰€ä»¥æ’åœ¨æœ€ä¸Šé¢æ­£å¥½
    marker = ""
    if marker in content:
        new_content = content.replace(marker, marker + "\n" + new_html)
        with open("index.html", "w", encoding="utf-8") as f:
            f.write(new_content)
        print("âœ… ç½‘é¡µå­˜æ¡£å·²æ›´æ–° (æœ€æ–°æ–°é—»åœ¨é¡¶éƒ¨)")

def send_grouped_card(source_name, news_list):
    """å‘é€èšåˆå¡ç‰‡"""
    if not FEISHU_WEBHOOK or not news_list: return

    headers = {"Content-Type": "application/json"}
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "template": "orange", 
            "title": { "tag": "plain_text", "content": f"ğŸ“Š {source_name} ({len(news_list)}æ¡æ–°æ¶ˆæ¯)" }
        },
        "elements": []
    }

    for i, news in enumerate(news_list):
        element_div = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ”¹ **{news['title_cn']}**\nğŸ“„ åŸæ–‡ï¼š[{news['title']}]({news['link']})\nâ° æ—¶é—´ï¼š{news['display_time']}"
            }
        }
        card_content["elements"].append(element_div)
        if i < len(news_list) - 1:
            card_content["elements"].append({"tag": "hr"})

    card_content["elements"].append({"tag": "hr"})
    card_content["elements"].append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": f"æ¥è‡ªï¼š{KEYWORD} æœºå™¨äºº | è‡ªåŠ¨èšåˆæ¨¡å¼"}]
    })

    try:
        requests.post(FEISHU_WEBHOOK, headers=headers, data=json.dumps({"msg_type": "interactive", "card": card_content}))
        print(f"âœ… [èšåˆæ¨é€] {source_name} - {len(news_list)} æ¡å†…å®¹å·²å‘é€")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def fetch_news_from_url(url):
    collected_news = []
    print(f"ğŸ” æ£€æŸ¥: {url}")
    try:
        feed = feedparser.parse(url, agent="Mozilla/5.0")
        if not feed.entries: return []
        
        feed_title = feed.feed.get('title', 'Market')
        # æ¥æºåˆ¤æ–­é€»è¾‘
        if "Bloomberg" in feed_title:
            if "Market" in feed_title: source_name = "å½­åšå¸‚åœº"
            elif "Economics" in feed_title: source_name = "å½­åšç»æµ"
            elif "Tech" in feed_title: source_name = "å½­åšç§‘æŠ€"
            else: source_name = "å½­åšç¤¾"
        elif "Investing" in feed_title: source_name = "è‹±ä¸ºè´¢æƒ…"
        elif "Reuters" in feed_title: source_name = "è·¯é€ç¤¾"
        elif "36Kr" in feed_title: source_name = "36æ°ª"
        elif "TechCrunch" in feed_title: source_name = "TechCrunch"
        else: source_name = feed_title[:10].replace("RSS", "").strip()

        for entry in feed.entries[:5]:
            title_origin = entry.title
            link = entry.link
            published_time = entry.published_parsed if hasattr(entry, 'published_parsed') else time.gmtime()
            pub_dt = datetime.fromtimestamp(time.mktime(published_time), timezone.utc)
            
            if pub_dt > (datetime.now(timezone.utc) - timedelta(minutes=TIME_WINDOW_MINUTES)):
                if is_work_time():
                    news_item = {
                        "title": title_origin,
                        "link": link,
                        "pub_dt": pub_dt,
                        "display_time": (pub_dt + timedelta(hours=8)).strftime('%H:%M'),
                        "source": source_name,
                        "title_cn": "" # ç¨åç»Ÿä¸€å¡«
                    }
                    collected_news.append(news_item)
    except Exception as e: 
        print(f"Error: {e}")
    
    return collected_news

if __name__ == "__main__":
    if not RSS_LIST:
        print("âš ï¸ é…ç½®ç¼ºå¤±: è¯·æ£€æŸ¥ rss.txt")
    else:
        print("ğŸ“¥ å¼€å§‹æŠ“å–...")
        all_news_buffer = []
        for rss_url in RSS_LIST:
            news_list = fetch_news_from_url(rss_url)
            all_news_buffer.extend(news_list)

        # æ’åºï¼šå…ˆæŒ‰ã€æ—§ -> æ–°ã€‘æ’å¥½
        # ä¸ºä»€ä¹ˆè¦æ—§åˆ°æ–°ï¼Ÿå› ä¸ºé£ä¹¦å¡ç‰‡é‡Œè¯»èµ·æ¥ä¹ æƒ¯æ˜¯ä»ä¸Šå¾€ä¸‹è¯»
        all_news_buffer.sort(key=lambda x: x['pub_dt'])
        
        if all_news_buffer:
            print(f"âš¡ æ­£åœ¨å¤„ç† {len(all_news_buffer)} æ¡æ–°é—» (ç¿»è¯‘ä¸­)...")
            # ç»Ÿä¸€ç¿»è¯‘
            for news in all_news_buffer:
                news['title_cn'] = translate_text(news['title'])

            # === åŠ¨ä½œ 1: æ›´æ–°ç½‘é¡µå­˜æ¡£ (å€’åº) ===
            # è¿™é‡Œç”¨äº† reversed()ï¼ŒæŠŠåˆ—è¡¨å˜æˆã€æ–° -> æ—§ã€‘ï¼Œä»è€Œå®ç°æœ€æ–°æ–°é—»åœ¨ç½‘é¡µæœ€é¡¶éƒ¨
            update_html_archive(reversed(all_news_buffer))

            # === åŠ¨ä½œ 2: å‘é€é£ä¹¦èšåˆå¡ç‰‡ ===
            # è¿™é‡Œçš„ all_news_buffer ä¾ç„¶æ˜¯ã€æ—§ -> æ–°ã€‘ï¼Œç¬¦åˆé˜…è¯»ä¹ æƒ¯
            news_by_source = {}
            for news in all_news_buffer:
                source = news['source']
                if source not in news_by_source: news_by_source[source] = []
                news_by_source[source].append(news)
            
            for source, news_list in news_by_source.items():
                send_grouped_card(source, news_list)
                time.sleep(1)
        else:
            print("ğŸ“­ æ— æ–°æ¶ˆæ¯")
