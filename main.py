import feedparser
import requests
import json
import time
import os
import re
from datetime import datetime, timedelta, timezone
from deep_translator import GoogleTranslator

# ================= é…ç½®åŒº =================
FEISHU_WEBHOOK = os.environ.get("FEISHU_WEBHOOK")
KEYWORD = "ç›‘æ§"

# 1. æ­£å¸¸è¿è¡Œæ¨¡å¼ (åªçœ‹è¿‡å»16åˆ†é’Ÿ)
TIME_WINDOW_MINUTES = 16

# 2. ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¿ç•™ä¸€å‘¨å·¦å³çš„æ•°æ®é‡
# æ¯å¤©çº¦60æ¬¡è¿è¡Œ * 7å¤© * æ¯æ¬¡å¹³å‡2æ¡ = 840æ¡
# è®¾å®šä¸º 800ï¼Œæ–‡ä»¶å¤§å°ä»…çº¦ 300KBï¼Œéå¸¸å®‰å…¨
MAX_ARCHIVE_ITEMS = 800 

def load_rss_list():
    rss_list = []
    if os.path.exists("rss.txt"):
        with open("rss.txt", "r", encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    rss_list.append(line)
    return rss_list

RSS_LIST = load_rss_list()
# =========================================

def is_work_time():
    utc_now = datetime.now(timezone.utc)
    beijing_time = utc_now + timedelta(hours=8)
    if 8 <= beijing_time.hour < 22:
        return True
    return False

def translate_text(text):
    try:
        for char in text:
            if '\u4e00' <= char <= '\u9fff': return text
        translator = GoogleTranslator(source='auto', target='zh-CN')
        return translator.translate(text)
    except: return text

# --- ç½‘é¡µå†™å…¥å‡½æ•° (å¸¦è‡ªåŠ¨æ¸…ç†) ---
def update_html_archive(news_list):
    if not os.path.exists("index.html"): return
    
    # 1. ç”Ÿæˆæ–°å†…å®¹çš„ HTML
    new_html = ""
    for news in news_list:
        item = f"""
        <div class="timeline-item">
            <div class="time-label">{news['display_time']}</div>
            <div class="dot"></div>
            <a href="{news['link']}" target="_blank" class="content-card">
                <span class="source-badge">{news['source']}</span>
                <h3 class="news-title">{news['title_cn']}</h3>
                <div class="news-origin">{news['title']}</div>
            </a>
        </div>
        """
        new_html += item

    # 2. è¯»å–æ–‡ä»¶
    with open("index.html", "r", encoding="utf-8") as f:
        content = f.read()
    
    # 3. æ’å…¥æ–°å†…å®¹
    marker = ""
    if marker in content:
        content = content.replace(marker, marker + "\n" + new_html)
        
        # === 4. æ¸…ç†æ—§æ–°é—» (æ§åˆ¶åœ¨ä¸€å‘¨å·¦å³) ===
        # æŸ¥æ‰¾æ‰€æœ‰çš„ timeline-item
        item_matches = [m.start() for m in re.finditer(r'<div class="timeline-item">', content)]
        
        # å¦‚æœè¶…è¿‡é™åˆ¶ (800æ¡)
        if len(item_matches) > MAX_ARCHIVE_ITEMS:
            print(f"ğŸ§¹ è§¦å‘æ¸…ç†: å½“å‰ {len(item_matches)} æ¡ï¼Œä¿ç•™æœ€æ–°çš„ {MAX_ARCHIVE_ITEMS} æ¡")
            
            # æ‰¾åˆ°ç¬¬ 801 æ¡çš„å¼€å§‹ä½ç½®ï¼ŒæŠŠåé¢çš„åˆ‡æ‰
            cut_off_index = item_matches[MAX_ARCHIVE_ITEMS]
            kept_content = content[:cut_off_index]
            
            # è¡¥å…¨é¡µè„š
            footer = """
        </div>
        <div style="text-align: center; margin-top: 50px; color: var(--text-sub); font-size: 0.8rem;">
            â€”â€” End of Archive (Last 7 Days) â€”â€”
        </div>
    </div>
</body>
</html>"""
            content = kept_content + footer
            
        with open("index.html", "w", encoding="utf-8") as f:
            f.write(content)
        print("âœ… ç½‘é¡µå·²æ›´æ–°")

def send_grouped_card(source_name, news_list):
    if not FEISHU_WEBHOOK or not news_list: return

    headers = {"Content-Type": "application/json"}
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "template": "orange", 
            "title": { "tag": "plain_text", "content": f"ğŸ“Š {source_name} ({len(news_list)}æ¡æ–°æ¶ˆæ¯)" }
        },
        "elements": []
    }

    for i, news in enumerate(news_list):
        element_div = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ”¹ **{news['title_cn']}**\nğŸ“„ åŸæ–‡ï¼š[{news['title']}]({news['link']})\nâ° æ—¶é—´ï¼š{news['display_time']}"
            }
        }
        card_content["elements"].append(element_div)
        if i < len(news_list) - 1:
            card_content["elements"].append({"tag": "hr"})

    card_content["elements"].append({"tag": "hr"})
    card_content["elements"].append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": f"æ¥è‡ªï¼š{KEYWORD} æœºå™¨äºº | è‡ªåŠ¨èšåˆæ¨¡å¼"}]
    })

    try:
        requests.post(FEISHU_WEBHOOK, headers=headers, data=json.dumps({"msg_type": "interactive", "card": card_content}))
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def fetch_news_from_url(url):
    collected_news = []
    print(f"ğŸ” æ£€æŸ¥: {url}")
    try:
        feed = feedparser.parse(url, agent="Mozilla/5.0")
        if not feed.entries: return []
        
        feed_title = feed.feed.get('title', 'Market')
        # ç®€å•æ¥æºåˆ¤æ–­
        if "Bloomberg" in feed_title:
            if "Market" in feed_title: source_name = "å½­åšå¸‚åœº"
            elif "Economics" in feed_title: source_name = "å½­åšç»æµ"
            elif "Tech" in feed_title: source_name = "å½­åšç§‘æŠ€"
            else: source_name = "å½­åšç¤¾"
        elif "Investing" in feed_title: source_name = "è‹±ä¸ºè´¢æƒ…"
        elif "Reuters" in feed_title: source_name = "è·¯é€ç¤¾"
        elif "36Kr" in feed_title: source_name = "36æ°ª"
        elif "TechCrunch" in feed_title: source_name = "TechCrunch"
        else: source_name = feed_title[:10].replace("RSS", "").strip()

        for entry in feed.entries[:5]:
            title_origin = entry.title
            link = entry.link
            published_time = entry.published_parsed if hasattr(entry, 'published_parsed') else time.gmtime()
            pub_dt = datetime.fromtimestamp(time.mktime(published_time), timezone.utc)
            
            if pub_dt > (datetime.now(timezone.utc) - timedelta(minutes=TIME_WINDOW_MINUTES)):
                if is_work_time():
                    news_item = {
                        "title": title_origin,
                        "link": link,
                        "pub_dt": pub_dt,
                        "display_time": (pub_dt + timedelta(hours=8)).strftime('%H:%M'),
                        "source": source_name,
                        "title_cn": "" 
                    }
                    collected_news.append(news_item)
    except Exception as e: 
        print(f"Error: {e}")
    
    return collected_news

if __name__ == "__main__":
    if not RSS_LIST:
        print("âš ï¸ é…ç½®ç¼ºå¤±")
    else:
        print("ğŸ“¥ å¼€å§‹æŠ“å–...")
        all_news_buffer = []
        for rss_url in RSS_LIST:
            news_list = fetch_news_from_url(rss_url)
            all_news_buffer.extend(news_list)

        all_news_buffer.sort(key=lambda x: x['pub_dt'])
        
        if all_news_buffer:
            print(f"âš¡ æ­£åœ¨å¤„ç† {len(all_news_buffer)} æ¡æ–°é—»...")
            for news in all_news_buffer:
                news['title_cn'] = translate_text(news['title'])

            # åŠ¨ä½œ1: å€’åºå†™ç½‘é¡µ (é™åˆ¶800æ¡)
            update_html_archive(reversed(all_news_buffer))

            # åŠ¨ä½œ2: å‘é€é£ä¹¦
            news_by_source = {}
            for news in all_news_buffer:
                source = news['source']
                if source not in news_by_source: news_by_source[source] = []
                news_by_source[source].append(news)
            
            for source, news_list in news_by_source.items():
                send_grouped_card(source, news_list)
                time.sleep(1)
        else:
            print("ğŸ“­ æ— æ–°æ¶ˆæ¯")
