import feedparser
import requests
import json
import time
import os
from datetime import datetime, timedelta, timezone
from deep_translator import GoogleTranslator

# ================= é…ç½®åŒº =================
FEISHU_WEBHOOK = os.environ.get("FEISHU_WEBHOOK")
KEYWORD = "ç›‘æ§"

# âš ï¸ æµ‹è¯•å®Œè®°å¾—æŠŠè¿™ä¸ªæ”¹å› 16
TIME_WINDOW_MINUTES = 1440 

# åŠ è½½è®¢é˜…æº
def load_rss_list():
    rss_list = []
    if os.path.exists("rss.txt"):
        with open("rss.txt", "r", encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    rss_list.append(line)
    return rss_list

RSS_LIST = load_rss_list()
# =========================================

def is_work_time():
    utc_now = datetime.now(timezone.utc)
    beijing_time = utc_now + timedelta(hours=8)
    if 8 <= beijing_time.hour < 22:
        return True
    return False

def translate_text(text):
    try:
        for char in text:
            if '\u4e00' <= char <= '\u9fff': return text
        translator = GoogleTranslator(source='auto', target='zh-CN')
        return translator.translate(text)
    except: return text

def send_grouped_card(source_name, news_list):
    """
    å‘é€èšåˆå¡ç‰‡ï¼šä¸€å¼ å¡ç‰‡åŒ…å«å¤šæ¡æ–°é—»
    """
    if not FEISHU_WEBHOOK: return
    if not news_list: return

    headers = {"Content-Type": "application/json"}
    
    # 1. æ„å»ºå¡ç‰‡å¤´éƒ¨ (Header)
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "template": "orange", 
            "title": {
                "tag": "plain_text", 
                "content": f"ğŸ“Š {source_name} ({len(news_list)}æ¡æ–°æ¶ˆæ¯)"
            }
        },
        "elements": []
    }

    # 2. åŠ¨æ€æ„å»ºä¸­é—´çš„æ–°é—»åˆ—è¡¨ (Elements)
    # å¾ªç¯æŠŠæ¯ä¸€æ¡æ–°é—»åŠ è¿›å»
    for i, news in enumerate(news_list):
        # æ¯ä¸€æ¡æ–°é—»æ˜¯ä¸€ä¸ª div
        element_div = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ”¹ **{news['title_cn']}**\nğŸ“„ åŸæ–‡ï¼š[{news['title']}]({news['link']})\nâ° æ—¶é—´ï¼š{news['display_time']}"
            }
        }
        card_content["elements"].append(element_div)
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¡ï¼ŒåŠ ä¸€ä¸ªåˆ†å‰²çº¿ï¼Œå¥½çœ‹ä¸€ç‚¹
        if i < len(news_list) - 1:
            card_content["elements"].append({"tag": "hr"})

    # 3. åº•éƒ¨ç½²å
    card_content["elements"].append({"tag": "hr"})
    card_content["elements"].append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": f"æ¥è‡ªï¼š{KEYWORD} æœºå™¨äºº | è‡ªåŠ¨èšåˆæ¨¡å¼"}]
    })

    try:
        requests.post(FEISHU_WEBHOOK, headers=headers, data=json.dumps({"msg_type": "interactive", "card": card_content}))
        print(f"âœ… [èšåˆæ¨é€] {source_name} - {len(news_list)} æ¡å†…å®¹å·²å‘é€")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def fetch_news_from_url(url):
    collected_news = []
    print(f"ğŸ” æ£€æŸ¥: {url}")
    try:
        feed = feedparser.parse(url, agent="Mozilla/5.0")
        if not feed.entries: return []
        
        # æ¥æºè¯†åˆ«
        feed_title = feed.feed.get('title', 'Market')
        if "Bloomberg" in feed_title:
            if "Market" in feed_title: source_name = "å½­åšå¸‚åœº"
            elif "Economics" in feed_title: source_name = "å½­åšç»æµ"
            elif "Tech" in feed_title: source_name = "å½­åšç§‘æŠ€"
            else: source_name = "å½­åšç¤¾"
        elif "Investing" in feed_title: source_name = "è‹±ä¸ºè´¢æƒ…"
        elif "Reuters" in feed_title: source_name = "è·¯é€ç¤¾"
        elif "36Kr" in feed_title: source_name = "36æ°ª"
        elif "TechCrunch" in feed_title: source_name = "TechCrunch"
        else: source_name = feed_title[:10].replace("RSS", "").strip()

        for entry in feed.entries[:5]:
            title_origin = entry.title
            link = entry.link
            published_time = entry.published_parsed if hasattr(entry, 'published_parsed') else time.gmtime()
            pub_dt = datetime.fromtimestamp(time.mktime(published_time), timezone.utc)
            
            if pub_dt > (datetime.now(timezone.utc) - timedelta(minutes=TIME_WINDOW_MINUTES)):
                if is_work_time():
                    news_item = {
                        "title": title_origin,
                        "link": link,
                        "pub_dt": pub_dt,
                        "display_time": (pub_dt + timedelta(hours=8)).strftime('%H:%M'), # èšåˆæ¨¡å¼ä¸‹ï¼Œæ—¶é—´åªæ˜¾ç¤º æ—¶:åˆ† å°±å¤Ÿäº†
                        "source": source_name,
                        "title_cn": "" 
                    }
                    collected_news.append(news_item)
    except Exception as e: 
        print(f"Error: {e}")
    
    return collected_news

if __name__ == "__main__":
    if not FEISHU_WEBHOOK or not RSS_LIST:
        print("âš ï¸ é…ç½®ç¼ºå¤±")
    else:
        print("ğŸ“¥ å¼€å§‹æŠ“å–...")
        all_news_buffer = []
        
        # 1. æŠ“å–æ‰€æœ‰æ–°é—»
        for rss_url in RSS_LIST:
            news_list = fetch_news_from_url(rss_url)
            all_news_buffer.extend(news_list)

        # 2. åˆ†ç»„é€»è¾‘ (Grouping)
        # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼š { "å½­åšå¸‚åœº": [æ–°é—»1, æ–°é—»2], "36æ°ª": [æ–°é—»A] }
        news_by_source = {}
        
        # å…ˆæŒ‰æ—¶é—´æ’ä¸ªåºï¼Œä¿è¯å¡ç‰‡é‡Œçš„æ–°é—»æ˜¯ä»æ—§åˆ°æ–°çš„
        all_news_buffer.sort(key=lambda x: x['pub_dt'])

        for news in all_news_buffer:
            source = news['source']
            if source not in news_by_source:
                news_by_source[source] = []
            news_by_source[source].append(news)

        # 3. æŒ‰æ¥æºå‘é€èšåˆå¡ç‰‡
        if not news_by_source:
            print("ğŸ“­ æš‚æ— æ–°æ¶ˆæ¯")
        else:
            for source, news_list in news_by_source.items():
                print(f"ğŸ“¦ æ­£åœ¨æ‰“åŒ… {source} çš„ {len(news_list)} æ¡æ–°é—»...")
                
                # ç»Ÿä¸€ç¿»è¯‘ (æ”¾åœ¨å‘é€å‰ç¿»è¯‘)
                for news in news_list:
                    news['title_cn'] = translate_text(news['title'])
                
                # å‘é€è¿™ä¸€ç»„
                send_grouped_card(source, news_list)
                time.sleep(1) # é˜²æ­¢å‘å¤ªå¿«
