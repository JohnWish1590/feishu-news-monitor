import feedparser
import requests
import json
import time
import os
from datetime import datetime, timedelta, timezone
from deep_translator import GoogleTranslator

# ================= é…ç½®åŒº =================
FEISHU_WEBHOOK = os.environ.get("FEISHU_WEBHOOK")
KEYWORD = "ç›‘æ§"

# ã€æµ‹è¯•æ¨¡å¼ã€‘å½“å‰è®¾ä¸º 1440 (24å°æ—¶) ä»¥ä¾¿æ‚¨çœ‹åˆ°æ•ˆæœ
# âš ï¸ æ­£å¼ä½¿ç”¨æ—¶è¯·æ”¹å› 16
TIME_WINDOW_MINUTES = 1440 

# ä» rss.txt åŠ è½½åˆ—è¡¨
def load_rss_list():
    rss_list = []
    if os.path.exists("rss.txt"):
        with open("rss.txt", "r", encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    rss_list.append(line)
    return rss_list

RSS_LIST = load_rss_list()
# =========================================

def is_work_time():
    utc_now = datetime.now(timezone.utc)
    beijing_time = utc_now + timedelta(hours=8)
    if 8 <= beijing_time.hour < 22:
        return True
    return False

def translate_text(text):
    try:
        for char in text:
            if '\u4e00' <= char <= '\u9fff': return text
        translator = GoogleTranslator(source='auto', target='zh-CN')
        return translator.translate(text)
    except: return text

def send_feishu_card(news_item):
    """
    å‘é€å•æ¡æ¶ˆæ¯ï¼Œå‚æ•°æ˜¯å­—å…¸å¯¹è±¡
    """
    if not FEISHU_WEBHOOK: return
    
    # è§£åŒ…æ•°æ®
    title_en = news_item['title']
    title_cn = news_item['title_cn']
    link = news_item['link']
    date_str = news_item['display_time']
    source_name = news_item['source']

    headers = {"Content-Type": "application/json"}
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "template": "orange", 
            "title": {"tag": "plain_text", "content": f"ã€{source_name}ã€‘ {title_cn}"}
        },
        "elements": [
            {"tag": "div", "text": {"tag": "lark_md", "content": f"**åŸæ–‡ï¼š** {title_en}\n**æ—¶é—´ï¼š** {date_str}"}},
            {"tag": "hr"},
            {"tag": "action", "actions": [{"tag": "button", "text": {"tag": "plain_text", "content": "ç‚¹å‡»æŸ¥çœ‹å…¨æ–‡"}, "type": "primary", "url": link}]},
            {"tag": "note", "elements": [{"tag": "plain_text", "content": f"æ¥è‡ªï¼š{KEYWORD} æœºå™¨äºº"}]}
        ]
    }
    try:
        requests.post(FEISHU_WEBHOOK, headers=headers, data=json.dumps({"msg_type": "interactive", "card": card_content}))
        print(f"âœ… æ¨é€æˆåŠŸ: {title_cn[:10]}...")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def fetch_news_from_url(url):
    """
    åªæŠ“å–ï¼Œä¸å‘é€ã€‚è¿”å›æŠ“å–åˆ°çš„æ–°é—»åˆ—è¡¨ã€‚
    """
    collected_news = []
    print(f"ğŸ” æ­£åœ¨æ£€æŸ¥: {url}")
    try:
        feed = feedparser.parse(url, agent="Mozilla/5.0")
        if not feed.entries: return []
        
        # æ¥æºè¯†åˆ«
        feed_title = feed.feed.get('title', 'Market')
        if "Bloomberg" in feed_title:
            if "Market" in feed_title: source_name = "å½­åšå¸‚åœº"
            elif "Economics" in feed_title: source_name = "å½­åšç»æµ"
            elif "Tech" in feed_title: source_name = "å½­åšç§‘æŠ€"
            else: source_name = "å½­åšç¤¾"
        elif "Investing" in feed_title: source_name = "è‹±ä¸ºè´¢æƒ…"
        elif "Reuters" in feed_title: source_name = "è·¯é€ç¤¾"
        elif "36Kr" in feed_title: source_name = "36æ°ª"
        elif "Huxiu" in feed_title: source_name = "è™å—…"
        else: source_name = feed_title[:10].replace("RSS", "").strip()

        for entry in feed.entries[:5]:
            title_origin = entry.title
            link = entry.link
            published_time = entry.published_parsed if hasattr(entry, 'published_parsed') else time.gmtime()
            pub_dt = datetime.fromtimestamp(time.mktime(published_time), timezone.utc)
            
            # æ—¶é—´è¿‡æ»¤
            if pub_dt > (datetime.now(timezone.utc) - timedelta(minutes=TIME_WINDOW_MINUTES)):
                if is_work_time():
                    # è¿™é‡Œå…ˆä¸ç¿»è¯‘ï¼Œç­‰æ’åºåå†ç¿»è¯‘ï¼Œæˆ–è€…ç°åœ¨ç¿»è¯‘éƒ½å¯ä»¥
                    # ä¸ºäº†æ–¹ä¾¿ï¼Œå…ˆå­˜èµ·æ¥
                    news_item = {
                        "title": title_origin,
                        "link": link,
                        "pub_dt": pub_dt, # ç”¨äºæ’åºçš„åŸå§‹æ—¶é—´å¯¹è±¡
                        "display_time": (pub_dt + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S'),
                        "source": source_name,
                        "title_cn": "" # ç¨åå¡«å…¥
                    }
                    collected_news.append(news_item)
    except Exception as e: 
        print(f"Error checking {url}: {e}")
    
    return collected_news

if __name__ == "__main__":
    if not FEISHU_WEBHOOK:
        print("âš ï¸ æœªæ£€æµ‹åˆ° Webhook")
    elif not RSS_LIST:
        print("âš ï¸ rss.txt ä¸ºç©º")
    else:
        print("ğŸ“¥ å¼€å§‹æ”¶é›†æ‰€æœ‰è®¢é˜…æºçš„æ–°é—»...")
        all_news_buffer = []
        
        # 1. éå†æ‰€æœ‰ URLï¼Œæ”¶é›†æ–°é—»
        for rss_url in RSS_LIST:
            news_list = fetch_news_from_url(rss_url)
            all_news_buffer.extend(news_list)
            
        print(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(all_news_buffer)} æ¡ç¬¦åˆæ—¶é—´è¦æ±‚çš„æ–°é—»")

        # 2. æ ¸å¿ƒæ­¥éª¤ï¼šæŒ‰æ—¶é—´æ’åº
        # x['pub_dt'] æ˜¯æ—¶é—´å¯¹è±¡ã€‚ä»å°åˆ°å¤§æ’åº = ä»æ—§åˆ°æ–°ã€‚
        # è¿™æ ·é£ä¹¦é‡Œæœ€ä¸‹é¢çš„æ˜¯æœ€æ–°çš„ã€‚
        all_news_buffer.sort(key=lambda x: x['pub_dt'])

        # 3. é€æ¡ç¿»è¯‘å¹¶æ¨é€
        for news in all_news_buffer:
            # ç¿»è¯‘æ ‡é¢˜ (æ”¾åœ¨è¿™é‡Œæ˜¯ä¸ºäº†åªç¿»è¯‘æœ€ç»ˆè¦å‘çš„ï¼Œçœèµ„æº)
            print(f"âš¡ æ­£åœ¨å¤„ç†: [{news['source']}] {news['title'][:10]}...")
            news['title_cn'] = translate_text(news['title'])
            
            # å‘é€
            send_feishu_card(news)
            # ç¨å¾®åœé¡¿ä¸€ä¸‹ï¼Œé˜²æ­¢å‘å¤ªå¿«é¡ºåºä¹±äº†
            time.sleep(1)
            
        print("ğŸ æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
